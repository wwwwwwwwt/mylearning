<!--
 * @Author: zzzzztw
 * @Date: 2023-06-07 19:51:26
 * @LastEditors: Do not edit
 * @LastEditTime: 2023-06-16 16:45:34
 * @FilePath: /myLearning/项目总结/rpc.md
-->

# TinyRPC项目详解

## RPC框架解决什么问题

1. 相比http协议更加灵活，准确的说rpc不是一种协议，而是一种远程过程调用的方法，我们在此之上灵活使用我们自定义的协议格式，可以避免http比较冗长的报文，提高效率
2. 由于rpc更加灵活，所以更容易扩展和继承例如注册中心或负载均衡等功能
3. 相比于普通http的json，我们可以使用更高效的序列化协议，将文本转为2进制传输，获得更高性能。

## 写一个RPC框架需要考虑什么
1. 我们使用什么传输协议，用什么编码方式
2. 可用性问题：连接超时，或支持异步请求，负载均衡等。
3. 客户端并不关心服务端的地址和部署位置，只关心调用并得到结果，所以引出注册中心和负载均衡问题。

# 通信部分 

* 涉及codec.go gob.go client.go server.go

1. 客户端与服务端的通信需要协商一些内容，这里实现了 option + {header{server.method} | body} +  {header | body} ....的格式，首先使用json传递我们使用的密钥和解码方式，通过两次握手的形式，保证服务端处理完option后，客户端收到处理完成的消息后再发送header|body请求防止粘包，每进来一个新conn，就开一个协程去进行我们的处理逻辑。
2. 服务端处理后，使用读出的解码方式去对客户端发来的headr|body进行解析，通过解析请求头(按.来分割提供服务的对象和其具体调用的服务)
3. 解析完请求后，拿到服务对象和其具体调用的服务名。通过反射，拿到参数类型的返回值类型，然后通过报文拿到写入的参数（按顺序封装成结构体）通过 .method.Func.Call(服务对象实例，参数结构体，返回值结构体) 调用函数，得到返回值reply。

# 服务注册

* 涉及部分：service.go用于利用反射解析出传入方法对象的各种method和其参数与返回值，参数与返回值均为一个结构体；  server.go /register方法：用于保存方法名称，取得service解析出的服务对象调用其方法。

1. 维护了一个sync.map[服务名]服务实例， sync.map线程安全，使用空间换时间方法，底层由read和dirty两个哈希表来进行读写分离，提高效率，适用于读多写少场景。如果写多场景，会导致未命中次数miss字段数目增加，最终dirty map升级为read，这是on复杂度，效率会降低但仍是线程安全的。
2. 解析服务过后，会将其所有的具体的方法类型和方法名字存储到其内部哈希表中
3. 请求的消息会被分解为服务名，然后在这个哈希表中取到其实例。

# 请求处理：

* 涉及部分：server.go readrequest handlerequest findserver sendResponse；service.go: call

1. 在readrequest中会调用findserver函数，将请求FOO.SUM按逗号分割为对象和其方法，通过syncmap拿到对应的对象，在调用其内部哈希表，得到其具体的方法类型。
2. 之后readrequest函数内通过反射，拿到入参类型和返回参数的类型，最后组装起来返回一个请求的结构体。
3. 进入handlerequest，直接调用service的call方法，得到返回值。为了解决超时问题，利用了协程将调用call这一逻辑交给协程去处理，处理完了发送一个处理好的信号，主线程利用select和time.After阻塞等待，超过了时间将通过time.After结束select阻塞，返回错误。注意：这个协程发出去后如果一直阻塞，将不会回收其内存。大量错误可能会造成内存泄漏。
4. 得到结果后，最后通过sendrespone发会返回值，其中带有一个全局的锁，保证了发送消息的顺序性。

# 超时处理

* 涉及部分：client.go server.go
* 超时处理涉及地方：客户端：客户端与服务端建立连接超时，发送请求到服务端超时，等待服务处理超时， 从服务端接收响应，读报文超时。服务端：读客户端发来的请求超时，发送响应超时，调用方法映射超时。
* 具体代码再三个地方进行处理：客户端创建链接时，客户端调用Call时（包括发送-处理-接收报文所有的阶段），服务端处理报文超时即handleRequest逻辑。
* 具体就是，设定一个timeout和一个默认defaulttimeout，之后走两个逻辑，用户不提供timeout就走默认。
* 然后就是实现一个定时器，通过channel和time.After（超时时间）和select来控制超时逻辑。


# 支持http协议

* 涉及部分 client.go server.go
1. 原理：client端发送一个connect请求方法，客户端发现进来的链接是http方法，使用hijack()从连接中劫持conn，然后组装回复一个http报文，随后使用这个conn进行正常处理就好。

#  服务发现 + 负载均衡

* 涉及部分：discovery.go xclient.go discoveryrpc.go
* 实现了轮询算法和随机选择算法，会随机的从我们的注册的服务地址列表中，选择一个可用的服务地址返回给客户端。
* 这个注册中心维护了一个 [服务器地址 + 该服务上一次的心跳时间] 的 map，并且通过实现 http.Handler 接口，对外提供 Http 服务，这样每个服务器可以通过 POST 请求发送心跳或注册新的服务地址、服务发现模块通过 GET 请求拉取所有可用服务器的地址。
* 首先开启服务时会将我们的服务注册到多个服务端地址，服务端将地址再注册给服务中心，客户端感知服务中心，服务中心会将现在按照算法选择一个健康的节点返回给客户端进行调用。
* 服务端为了证明自己健康会通过HTTP的POST定时给注册中心发送心跳来更新自己的时间，服务中心定时采用refresh来发现节点是否超过时间，如果超时就会将其删除。客户端会通过GET定时的向服务中心拉去目前存活的节点，然后根据算法选择一个进行服务调用。

# 支持etcd
* 涉及部分：etcd.go discovery_etcd.go
* 利用clientV3接口实现与etcd实现交互，etcd默认监听2379端口
* 创建一个client对象，将其绑定到默认端口上传给client对象。把服务中心挂起。
* put操作是将我们服务的节点地址挂在etcd前缀的后面，利用租约Grant和put（withLeaseid）和设定一定的超时时间，将服务器绑定到监听对象中。并利用心跳给服务器续租。
* discovery_etcd服务发现部分继承了我们自己写的简单服务发现，思路就是利用Get从etcd监控的前缀中取出所有目前健康的节点地址，然后复用我们之前写的负载均衡的逻辑挑一个返回给客户端。利用了watch机制，目前只是实现了当节点绑定的前缀后的路径发生变化时，会进行一次全量更新，其实这里可以根据上层逻辑再细化，比如服务节点会加上他带的服务内容，这样这个服务增添新的服务时也会被watch捕捉到，进行更新操作。